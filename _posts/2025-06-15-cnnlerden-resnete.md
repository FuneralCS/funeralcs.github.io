---
title: "SÄ±fÄ±rdan CNNâ€™ler: LeNetâ€™ten ResNetâ€™e"
date: 2025-06-15 09:00:00 +0300
categories: [derin Ã¶ÄŸrenme, resnet, lenet, gÃ¶rÃ¼ntÃ¼ iÅŸleme]
tags: [derin Ã¶ÄŸrenme, resnet, lenet, yazÄ±lÄ±m, gÃ¶rÃ¼ntÃ¼ iÅŸleme]
author: tunahan
image:
  path: /assets/img/cv/cv.webp
description: "BugÃ¼n CNNâ€™lerin doÄŸuÅŸundan itibaren ResNet mimarisine nasÄ±l geldik. TÃ¼m bunlara deÄŸineceÄŸiz"
toc: true
math: false
mermaid: false
comments: true
pin: false
---
# SÄ±fÄ±rdan CNNâ€™ler: LeNetâ€™ten ResNetâ€™e

Herkese selamlar!  Bu yazÄ±da size bilgisayarlÄ± gÃ¶rÃ¼nÃ¼n (Computer Vision) ilkel yÃ¶ntemlerinden ResNet gibi derin Ã¶ÄŸrenme devlerine uzanan yolculuÄŸumuzu anlatacaÄŸÄ±m.

HiÃ§ teknik karmaÅŸaya girmeden, basitÃ§e konuÅŸacaÄŸÄ±z. Her ÅŸeyin en anlaÅŸÄ±lÄ±r hÃ¢liyle burada olduÄŸunu gÃ¶receksiniz!

> Not: Bu yazÄ± ResNetâ€™e kadar olan kÄ±smÄ± kapsamaktadÄ±r. DenseNet, EfficientNet, Vision Transformer gibi yeni mimarileri ilerleyen haftalarda iÅŸleyeceÄŸim.

##  BilgisayarlÄ± GÃ¶rÃ¼ Nedir?

BugÃ¼n hemen hemen her araÃ§ta ÅŸerit takip sistemi ve hatta otonom sÃ¼rÃ¼ÅŸ Ã¶zellikleri bulunuyor. Peki bu nasÄ±l mÃ¼mkÃ¼n oldu?

1960-1970â€™lerde ilk gÃ¶rÃ¼ntÃ¼ iÅŸleme denemeleri baÅŸladÄ±. O zamanlar gÃ¶rÃ¼ntÃ¼, sadece piksel dizileri olarak iÅŸleniyordu. Mesela gÃ¼nÃ¼mÃ¼zde kullandÄ±ÄŸÄ±mÄ±z 1080p gÃ¶rÃ¼ntÃ¼ler (RGB formatÄ±nda 3Ã—1920Ã—1080) devasa boyutlarda verilerdir. Ama ilk deneylerde MNIST gibi 28Ã—28 boyutunda, sadece siyah beyaz gÃ¶rÃ¼ntÃ¼ler kullanÄ±lÄ±yordu.

AmaÃ§ neydi? O dÃ¶nemlerde hedef Ã§ok daha basitti: gÃ¶rÃ¼ntÃ¼lerde sadece kenar, kÃ¶ÅŸe gibi temel Ã¶zellikleri tanÄ±mak.

###  Piksel Nedir?
Piksel, gÃ¶rÃ¼ntÃ¼nÃ¼n en kÃ¼Ã§Ã¼k yapÄ± birimi. RGB renk temasÄ±nda kÄ±rmÄ±zÄ± (R), yeÅŸil (G) ve mavi (B) kanallarÄ±ndan oluÅŸur. RGBâ€™den Ã¶nce ise grayscale (gri tonlama) kullanÄ±lÄ±yordu.

###  Histogram Nedir?
GÃ¶rÃ¼ntÃ¼deki parlaklÄ±k daÄŸÄ±lÄ±mÄ±nÄ±n Ã¶lÃ§Ã¼tÃ¼dÃ¼r. Bir fotoÄŸraftaki karanlÄ±k ve aydÄ±nlÄ±k alanlarÄ± gÃ¶sterir.

---

## Ä°lk Filtreler ve Kenar AlgÄ±lama

Ä°lk gÃ¶rÃ¼ntÃ¼ iÅŸleme filtreleri Sobel, Prewitt ve Roberts operatÃ¶rleriydi. Bu filtreler gÃ¶rÃ¼ntÃ¼ Ã¼zerindeki parlaklÄ±k farklarÄ±nÄ± belirleyerek "kenarlarÄ±" ortaya Ã§Ä±karÄ±yordu.

Bu iÅŸlem basitti:

1. GÃ¶rÃ¼ntÃ¼ gri tonlamaya Ã§evrilir.
2. 3Ã—3'lÃ¼k kÃ¼Ã§Ã¼k bir pencere (kernel) tÃ¼m gÃ¶rÃ¼ntÃ¼ Ã¼zerinde kaydÄ±rÄ±lÄ±r.
3. Her pikselde, Ã§evresinin aÄŸÄ±rlÄ±klÄ± toplamÄ± alÄ±narak kenar haritasÄ± oluÅŸturulur.

---

## Ã–zellik Ã‡Ä±karÄ±mÄ± ve Temsiller

Elle tasarlanan Ã§eÅŸitli teknikler vardÄ±:

- **Nokta Ã–zellikleri (Keypoints)**: 
  - **SIFT ve SURF**: GÃ¶rÃ¼ntÃ¼deki belirgin noktalarÄ± tespit eder ve bu noktalarÄ±n Ã¶zelliklerini Ã§Ä±karÄ±r.

- **HOG (Histogram of Oriented Gradients)**: 
  - GÃ¶rÃ¼ntÃ¼yÃ¼ kÃ¼Ã§Ã¼k hÃ¼crelere bÃ¶lÃ¼p her hÃ¼crede kenar yÃ¶nlerini histogramla tutar.
  - Ã–zellikle insan ve araÃ§ tanÄ±mada kullanÄ±lÄ±r.

- **Haar benzeri Ã¶zellikler**:
  - DikdÃ¶rtgen filtrelerle parlaklÄ±k farklarÄ± tespit edilir.
  - Violaâ€“Jones yÃ¼z tanÄ±ma algoritmasÄ±nda kullanÄ±ldÄ±.

Bu yÃ¶ntemlerin sorunu neydi? Hepsi insan eliyle tasarlanÄ±yordu. Bir gÃ¶rÃ¼ntÃ¼den elle filtrelerle Ã¶zellik Ã§Ä±karmak ve onlarÄ± SVM, k-NN, Random Forest gibi klasik makine Ã¶ÄŸrenme yÃ¶ntemleriyle sÄ±nÄ±flandÄ±rmak gerekiyordu. Bu, zor ve sÄ±nÄ±rlayÄ±cÄ±ydÄ±.

---

## Derin Ã–ÄŸrenme ve LeNetâ€™in DoÄŸuÅŸu

**Ä°htiyaÃ§:** Elle Ã¶zellik Ã§Ä±karma yerine, Ã¶zellikleri otomatik Ã¶ÄŸrenen sistemler geliÅŸtirmek. Ä°ÅŸte bu yÃ¼zden **CNNâ€™ler (Convolutional Neural Networks)** doÄŸdu.

**AmaÃ§:** MNIST veri setindeki 60 bin el yazÄ±sÄ± rakamÄ± otomatik sÄ±nÄ±flandÄ±rmak.

---

##  Derin Ã–ÄŸrenme Nedir?

Derin Ã¶ÄŸrenme, insan beyninin Ã§alÄ±ÅŸma prensiplerinden esinlenilerek geliÅŸtirilmiÅŸ yapay sinir aÄŸlarÄ±nÄ± temel alan makine Ã¶ÄŸrenmesinin bir alt alanÄ±dÄ±r.

1940â€™larda baÅŸlayan yapay sinir aÄŸÄ± Ã§alÄ±ÅŸmalarÄ±, 1980â€™lerde geri yayÄ±lÄ±m (backpropagation) algoritmasÄ±nÄ±n bulunmasÄ±yla hÄ±z kazandÄ±. 2000â€™lerde bÃ¼yÃ¼k verinin ve gÃ¼Ã§lÃ¼ donanÄ±mlarÄ±n ortaya Ã§Ä±kmasÄ±yla gerÃ§ek anlamda derin Ã¶ÄŸrenme dÃ¶nemi baÅŸladÄ±.

Temel derin Ã¶ÄŸrenme tÃ¼rleri:

- Yapay Sinir AÄŸlarÄ± (ANN)
- EvriÅŸimli Sinir AÄŸlarÄ± (CNN) â€“ BugÃ¼n konuÅŸtuÄŸumuz
- Tekrarlayan Sinir AÄŸlarÄ± (RNN)
- Uzun KÄ±sa Vadeli Bellek (LSTM)
- Ãœretken Ã‡ekiÅŸmeli AÄŸlar (GAN)

Derin Ã¶ÄŸrenmede kullanÄ±lan teknikler:

- Geri yayÄ±lÄ±m (Backpropagation)
- Aktivasyon fonksiyonlarÄ± (ReLU, sigmoid, vb.)
- Dropout ve dÃ¼zenleÅŸtirme
- Stokastik Gradyan Ä°niÅŸi (SGD), Adam optimizasyonu vb.

Bu arada not dÃ¼ÅŸelim: Yapay sinir aÄŸlarÄ±nÄ±n babasÄ± sayÄ±lan Geoffrey Hinton, 2024 yÄ±lÄ±nda Nobel Fizik Ã–dÃ¼lÃ¼â€™nÃ¼ aldÄ±.

---

## LeNet-5 Mimarisi (1998)

LeNet, CNN'lerin atasÄ±dÄ±r. KatmanlarÄ±nÄ± yakÄ±ndan tanÄ±yalÄ±m:

- **KonvolÃ¼syon**: GÃ¶rÃ¼ntÃ¼ Ã¼zerinde kÃ¼Ã§Ã¼k filtreler gezdirilerek desenler yakalanÄ±r.
- **Aktivasyon (ReLU)**: Negatif deÄŸerleri sÄ±fÄ±rlayarak doÄŸrusal olmayanlÄ±k saÄŸlar.
- **Normalizasyon (BatchNorm)**: AÄŸÄ±n daha hÄ±zlÄ± ve dengeli Ã¶ÄŸrenmesini saÄŸlar.
- **Havuzlama (Pooling)**: Boyut kÃ¼Ã§Ã¼ltÃ¼lerek hesaplama maliyetini azaltÄ±r ve Ã¶zet bilgi Ã¼retir.
- **Tam BaÄŸlÄ± (Fully Connected)**: Son aÅŸamada Ã¶zellikleri sÄ±nÄ±flandÄ±rÄ±r.
> Convolution: GÃ¶rÃ¼ntÃ¼deki lokal desenleri bulmak iÃ§in kayan pencere.
> Stride: Filtrenin kaÃ§ piksel atlayarak hareket ettiÄŸini belirtir.
> Padding: GÃ¶rÃ¼ntÃ¼nÃ¼n kenarlarÄ±na sÄ±fÄ±r eklenerek boyutun korunmasÄ± saÄŸlanÄ±r.
> Flatten: 2D veriyi 1D hale getirerek Fully Connected katmana geÃ§iÅŸ saÄŸlar.

**LeNet YapÄ±sÄ±:**
- GiriÅŸ: 32Ã—32 gri gÃ¶rÃ¼ntÃ¼
- 2Ã—(KonvolÃ¼syon + Havuzlama) â†’ Tam baÄŸlÄ± katmanlar â†’ Softmax (10 sÄ±nÄ±f)
- MNIST verisinde yÃ¼ksek doÄŸruluk saÄŸladÄ± ve CNN dÃ¶nemini baÅŸlattÄ±.

> LeNet: Ä°lk gerÃ§ek CNN, elle Ã§Ä±karÄ±lan Ã¶zellik dÃ¶nemini kapattÄ±.
> ResNet: Derinlik sorununu Ã§Ã¶zerek modern derin aÄŸlarÄ±n kapÄ±sÄ±nÄ± aÃ§tÄ±. Transformerâ€™lar bile bu anlayÄ±ÅŸtan beslendi. (Attention)

---

## LeNet SonrasÄ± Ã–nemli Modeller

### ğŸ”¸ **AlexNet (2012)**
Geoffrey Hinton danÄ±ÅŸmanlÄ±ÄŸÄ±nda Alex Krizhevsky tarafÄ±ndan geliÅŸtirildi. 60 milyon parametre iÃ§eriyordu. GPU kullanÄ±mÄ±nÄ± baÅŸlatÄ±p ImageNet yarÄ±ÅŸmasÄ±nÄ± ezici farkla kazanarak derin Ã¶ÄŸrenme devrimini baÅŸlattÄ±.

### ğŸ”¸ **ZFNet (2013)**
AlexNetâ€™in iyileÅŸtirilmiÅŸ versiyonu; filtre boyutlarÄ±nÄ± kÃ¼Ã§Ã¼ltÃ¼p aÄŸÄ±n Ã¶ÄŸrenme ÅŸekillerini gÃ¶rselleÅŸtirdi.

### ğŸ”¸ **VGGNet (2014)**
Sadece 3Ã—3 konvolÃ¼syonlar kullanarak 16â€“19 katmana ulaÅŸtÄ±. Ã‡ok parametre iÃ§erdi (~138 milyon) ama sadeliÄŸiyle Ã¼nlendi.

### ğŸ”¸ **GoogLeNet/Inception (2014)**
AynÄ± anda farklÄ± boyutlardaki filtreleri kullanarak hem derin hem geniÅŸ bir aÄŸ oluÅŸturdu. Parametre sayÄ±sÄ±nÄ± (~6 milyon) dÃ¼ÅŸÃ¼k tutmayÄ± baÅŸardÄ±.

---

## ResNet (2015): DerinliÄŸin GerÃ§ek Ã‡Ã¶zÃ¼mÃ¼

Derin aÄŸlarda Ã¶ÄŸrenme zorlaÅŸÄ±r. Ã‡Ã¼nkÃ¼ gradyan kaybolur veya bozulur. Ä°ÅŸte ResNet bunu Ã§Ã¶zdÃ¼:

Normalde bir katmanÄ±n Ã§Ä±ktÄ±sÄ± ÅŸÃ¶yle hesaplanÄ±r:

```
Ã‡Ä±ktÄ± = F(x)
```
ResNet ise Ã§Ä±ktÄ± ile girdi arasÄ±nda doÄŸrudan baÄŸlantÄ± kurar:
```
Ã‡Ä±ktÄ± = F(x) + x
```

Bu "skip connection" sayesinde aÄŸ, farklarÄ± Ã¶ÄŸrenmeye odaklanÄ±r.  
>  BasitÃ§e: ToplamayÄ± biliyorsan Ã§arpma Ã¶ÄŸrenmek daha kolaydÄ±r. Model iÃ§in de aynÄ±sÄ± geÃ§erli. Yani Ã§arpmayÄ± Ã¶ÄŸreteceksek Ã¶nce toplamayÄ± Ã¶ÄŸretiyoruz yani iki sayÄ±nÄ±n etkileÅŸimini, ardÄ±ndan Ã§arpmanÄ±n genelini artÄ±k baÄŸlantÄ± adÄ±yla modele tekrar veriyoruz.

**ResNet-18 YapÄ±sÄ±**:
- GiriÅŸ: RGB gÃ¶rÃ¼ntÃ¼ (3Ã—224Ã—224)
- Ä°lk katman: Conv â†’ BatchNorm â†’ ReLU â†’ MaxPool (64Ã—56Ã—56)
- ArdÄ±ndan 4 adet Residual Layer:
  - Layer1: 64 kanal
  - Layer2: 128 kanal
  - Layer3: 256 kanal
  - Layer4: 512 kanal
- Son katmanlar: AvgPool â†’ Flatten â†’ Fully Connected â†’ SonuÃ§

---

## SonuÃ§ ve Ã–zet

BilgisayarlÄ± gÃ¶rÃ¼de LeNet'ten ResNet'e gelene kadar uzun bir yol katettik. BugÃ¼n ulaÅŸtÄ±ÄŸÄ±mÄ±z nokta, yapay zeka devriminin gerÃ§ek anlamda baÅŸladÄ±ÄŸÄ± yerdir.

UmarÄ±m bu yazÄ± sizin iÃ§in aydÄ±nlatÄ±cÄ± ve Ã¶ÄŸretici olmuÅŸtur.

Bir sonraki yazÄ±da gÃ¶rÃ¼ÅŸmek Ã¼zere, hoÅŸÃ§a kalÄ±n! 
